# WebSocket Integration

The WebSocket integration you've mentioned is designed for real-time speech generation with a focus on dynamic, seamless voice output. It uses a bidirectional WebSocket connection, allowing for continuous generation of speech as you stream text. This approach eliminates the need for pre-generating speech and ensures a smoother, more natural flow of voice generation. 

!> In many real-time use cases, you do not have pre-prepared speech text. For instance, when you are dynamically generating input text with a large model. For such cases, our WebSocket interface supports streaming input.

The traditional generation method splits the text into chunks, generates speech for each part independently, and then stitches them together to form complete speech. While this works well, the resulting speech often has sudden tone changes and awkward rhythms when the parts are merged.

Example Use Case:
In a scenario where the text input is dynamically generated by a model (e.g., for a real-time conversation or interaction), the system streams the speech continuously, ensuring it sounds coherent and natural without the abrupt pauses that typically result from batch processing.

<p class="my-2">
<audio controls src="https://cdn.online-gpt.net/files/without_continuations.wav">
  Your browser does not support the audio element.
</audio>
</p>

Now, let's try using our API to generate the same speech from text. As you can hear, this output sounds seamless and natural.

<p class="my-2">
<audio controls src="https://cdn.online-gpt.net/files/continuations.wav">
  Your browser does not support the audio element.
</audio>
</p>

Here's an overview of how this works:

### Key Features:
1. **Contextual Speech Generation**:
   - Speech is generated in a continuous manner by maintaining a `context_id`, ensuring that subsequent inputs are contextually linked to one another for smooth transitions and voice consistency.
   
2. **Real-Time Streaming**:
   - Speech is streamed in chunks, meaning you can receive parts of the generated audio while the system is still processing other parts, minimizing latency.
   
3. **Multiple Requests**:
   - Multiple speech generation requests can be handled concurrently through the same WebSocket connection, improving efficiency.

4. **Response Types**:
   - **`chunk`**: Indicates an in-progress audio chunk.
   - **`done`**: Signals the completion of the speech generation.
   - **`error`**: Provides error details if something goes wrong.

## Understanding Context

This API creates a bidirectional WebSocket connection. The connection supports multiplexing, meaning you can send multiple requests and receive corresponding responses in parallel.

The bidirectional WebSocket API is built around context:

1. When you send a generation request, you pass a `context_id`. Consecutive inputs with the same `context_id` will generate continuous speech while maintaining a consistent human-like tone.

2. The model's responses include the `context_id` you sent, so you can match requests and responses.

Please read the [context usage guide](/contexts) for more details.

## Usage and Performance Guidelines

To achieve better performance, we recommend the following usage patterns:

1. **Perform multiple generations via a single WebSocket connection**, using the same context ID for each text generated by the same large model output.

2. **Connect to the WebSocket API before the first generation** to ensure there is no connection delay when starting speech generation.

3. **Each turn in a conversation should correspond to a unique context**. For example, if you are using our API to support a speech agent like a customer service phone assistant, each turn in the conversation should be a new context ID.

4. **Start a new context for user interruptions**: If the user interrupts the response from the speech agent during a phone call, enable a new context ID for the agent's new response.

## WebSocket Connection Handshake

`GET` wss://api.aishengyun.com/v1/audio/speech

For connection authentication, refer to [using API keys for authentication](/api-overview?id=using-api-keys-for-authentication).

## Sending Data

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| model_id | string | Yes | Model identifier, currently using `emotion-tts-v1` |
| transcript | string | Yes | The text content to convert |
| voice | object | Yes | Voice configuration object |
| output_format | object | Yes | Output format configuration |
| language | string | No | Language setting |
| context_id | string | Yes | Context ID for the request |
| continue | boolean | Yes | Indicates whether this request continues the context's speech generation |

#### voice Object

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| mode | string | Yes | Voice mode: `id` or `embedding` |
| id | string | Conditionally required | Required when mode is `id`, specifying a [preset voice](/voices) |
| embedding | float[] | Conditionally required | Required when mode is `embedding`, a 192-dimensional voice feature vector |

#### output_format Object

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| container | string | Yes | Container format: `raw`/`wav`/`mp3` |
| sample_rate | int | Yes | Sample rate: `8000`/`16000`/`22050`/`24000`/`32000`/`44100`/`48000` |
| encoding | string | Conditionally required | Required for non-MP3 formats: `pcm_s16le`/`pcm_mulaw`/`pcm_alaw` |
| bit_rate | int | Conditionally required | Required for MP3 format: `32000`/`64000`/`96000`/`128000`/`192000` |

#### language Parameter

Possible values:
- `auto`: Auto-detect
- `en`: English
- `zh`: Chinese
- `ja`: Japanese

## Canceling Context Requests

| Name | Type | Required | Description |
|------|------|----------|-------------|
| context_id | string | Yes | The ID of the context to cancel |
| cancel | bool | Yes | Whether to cancel the context and stop generating speech for it |

## Receiving Data

The data types received are categorized as `chunk`, `done`, and `error`, with `"type": "done"` distinguishing the type of data.

### chunk Data

| Name | Type | Description |
|------|------|-------------|
| type | string | Value is `chunk` |
| status_code | int | Corresponding HTTP status code |
| data | string | Base64-encoded audio stream |
| done | bool | Whether the context's generation task is complete |
| context_id | string | The context ID for the request |

### done Data

| Name | Type | Description |
|------|------|-------------|
| type | string | Value is `done` |
| status_code | int | Corresponding HTTP status code |
| done | bool | Whether the context's generation task is complete |
| context_id | string | The context ID for the request |

### error Data

| Name | Type | Description |
|------|------|-------------|
| type | string | Value is `error` |
| status_code | int | Corresponding HTTP status code |
| error | string | Corresponding error message |
| done | bool | Whether the context's generation task is complete |
| context_id | string | The context ID for the request |

Below are examples of sending and receiving data. You can also download [API examples](https://cdn.online-gpt.net/files/examples.zip) to try it yourself.

```json
// Sending
{"model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh","transcript": "Hello","context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","continue": true}
{"model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh","transcript": ", nice","context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","continue": true}
{"model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh","transcript": " to ","context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","continue": true}
// Receiving
{ "type": "chunk", "status_code": 206, "data": "base64-encoded audio data", "done": false, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072" }
{ "type": "chunk", "status_code": 206, "data": "base64-encoded audio data", "done": false, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072" }
// Sending
{"model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh","transcript": "meet","context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","continue": true}
{"model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh","transcript": " you.","context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","continue": true}
// Receiving
{ "type": "chunk", "status_code": 206, "data": "base64-encoded audio data", "done": false, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072" }
{ "type": "chunk", "status_code": 206, "data": "base64-encoded audio data", "done": false, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072" }
// Sending
{"transcript": "", "continue": false, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072","model_id": "emotion-tts-v1","voice": {"mode": "id","id": "en_female_orva"},"output_format": {"container": "raw","encoding": "pcm_s16le","sample_rate": 16000}, "language": "zh"}
// Receiving
{ "type":"done", "status_code":200, "done":true, "context_id": "09dde5c1-1ac9-4434-9860-b97f9a792072" }
```
